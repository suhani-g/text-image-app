# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPahy9yO124JoNsgVtY323JKfl0t5btK
"""



import torch
from diffusers import StableDiffusionPipeline
import os
from huggingface_hub import login
import getpass
from PIL import Image
#from IPython.display import display
import streamlit as st

hf_token = os.getenv("HF_TOKEN")
login(token=hf_token)

import torch
from diffusers import StableDiffusionPipeline

@st.cache_resource
def load_pipeline():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    pipe.enable_attention_slicing()
    return pipe, device

pipe, device = load_pipeline()

# UI
st.title("ðŸŽ¨ Text-to-Image Generator")
prompt = st.text_input("Enter your prompt:", "A dreamy forest landscape with glowing magical lights and soft mist")

if st.button("Generate"):
    with st.spinner("Generating image..."):
        # Monkey patch the scheduler to avoid out-of-bounds access
import types

original_get_prev_sample = pipe.scheduler._get_prev_sample

def safe_get_prev_sample(self, sample, timestep, prev_timestep, model_output):
    max_index = len(self.alphas_cumprod) - 1
    safe_timestep = min(max(timestep, 0), max_index)
    safe_prev_timestep = min(max(prev_timestep, 0), max_index)
    return original_get_prev_sample(sample, safe_timestep, safe_prev_timestep, model_output)

pipe.scheduler._get_prev_sample = types.MethodType(safe_get_prev_sample, pipe.scheduler)

        result = pipe(prompt)
        image = result.images[0]
        st.image(image, caption="Generated Image", use_column_width=True)
        # Optionally save
        image.save("image.png")
