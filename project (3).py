# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPahy9yO124JoNsgVtY323JKfl0t5btK
"""



import torch
from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
import os
from huggingface_hub import login
from PIL import Image
import streamlit as st
import types


hf_token = os.getenv("HF_TOKEN")
login(token=hf_token)

@st.cache_resource
def load_pipeline():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )
    
   
    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)
    pipe = pipe.to(device)
    pipe.enable_attention_slicing()
    
    original_get_prev_sample = pipe.scheduler._get_prev_sample
    def safe_get_prev_sample(self, sample, timestep, prev_timestep, model_output):
        max_index = len(self.alphas_cumprod) - 1
        safe_timestep = min(max(timestep, 0), max_index)
        safe_prev_timestep = min(max(prev_timestep, 0), max_index)
        return original_get_prev_sample(sample, safe_timestep, safe_prev_timestep, model_output)

    pipe.scheduler._get_prev_sample = types.MethodType(safe_get_prev_sample, pipe.scheduler)

    return pipe, device

pipe, device = load_pipeline()

st.title("ðŸŽ¨ Text-to-Image Generator")
prompt = st.text_input("Enter your prompt:", "A dreamy forest landscape with glowing magical lights and soft mist")

if st.button("Generate"):
    with st.spinner("Generating image..."):
        result = pipe(prompt, num_inference_steps=25, guidance_scale=7.5)
        image = result.images[0]
        st.image(image, caption="Generated Image", use_column_width=True)
        image.save("image.png")
